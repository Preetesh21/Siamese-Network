# -*- coding: utf-8 -*-
"""Quora_Question_Similarity.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QTS8uBle5VcxdLYt2FlwqPA9HFKwhhI3
"""

import os
import tensorflow as tf
if tf.test.gpu_device_name():
    print('GPU Available')
else:
    print("Please install GPU version of TF")

# Commented out IPython magic to ensure Python compatibility.
os.environ['KAGGLE_CONFIG_DIR'] = "/content/drive/My Drive/Kaggle"
# %cd /content/drive/My Drive/Kaggle

!kaggle competitions download -c quora-question-pairs

!pip uninstall -y kaggle
!pip install --upgrade pip
!pip install kaggle==1.5.6
!kaggle -v

!kaggle competitions download -c quora-question-pairs

!unzip 'quora-question-pairs.zip'

os.listdir()

!unzip 'train.csv.zip'

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import keras
from keras import backend as K
from keras.engine.topology import Layer
from tensorflow.python.keras.layers import *
from tensorflow.python.keras.models import Sequential, Model, load_model
from keras.callbacks import ModelCheckpoint
from keras.initializers import Constant
from keras.layers.merge import add

from keras.preprocessing import sequence
from keras.preprocessing.text import Tokenizer, text_to_word_sequence
from keras.utils import np_utils
from keras import regularizers
from keras.regularizers import l2

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import classification_report,confusion_matrix,accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.manifold import TSNE

import nltk
from nltk.tokenize import sent_tokenize,word_tokenize
nltk.download('stopwords')
from nltk.corpus import stopwords
nltk.download('wordnet')
from nltk.stem import WordNetLemmatizer
nltk.download('punkt')
import os

data=pd.read_csv('train.csv')
data.head()

data['text']=data['question1']+' '+data['question2']
data.fillna(' ',inplace=True)

sns.countplot(data.is_duplicate).set_title('Question Pairs')

data.isnull().sum()

data=data[:10000]

data

sw=[]
sw=data['text']
sw

stop_words = set(stopwords.words('english'))
ws=[]
for example_sent in sw:
  word_tokens = word_tokenize(example_sent) 
  filtered_sentence = [w for w in word_tokens if not w in stop_words]
  ws.append(filtered_sentence)

for i in range(len(ws)):
  ws[i]= [word for word in ws[i] if word.isalpha()]

lemma=WordNetLemmatizer()
for i in range(len(ws)):
  for j in range(len(ws[i])):
    ws[i][j]=lemma.lemmatize(ws[i][j])

data['text_number']=ws

sw=[]
sw=data['question1']
stop_words = set(stopwords.words('english'))
ws=[]
for example_sent in sw:
  word_tokens = word_tokenize(example_sent) 
  filtered_sentence = [w for w in word_tokens if not w in stop_words]
  ws.append(filtered_sentence)
for i in range(len(ws)):
  ws[i]= [word for word in ws[i] if word.isalpha()]

lemma=WordNetLemmatizer()
for i in range(len(ws)):
  for j in range(len(ws[i])):
    ws[i][j]=lemma.lemmatize(ws[i][j]) 

data['question1_number']=ws

sw=[]
sw=data['question2']
stop_words = set(stopwords.words('english'))
ws=[]
for example_sent in sw:
  word_tokens = word_tokenize(example_sent) 
  filtered_sentence = [w for w in word_tokens if not w in stop_words]
  ws.append(filtered_sentence)
for i in range(len(ws)):
  ws[i]= [word for word in ws[i] if word.isalpha()]

lemma=WordNetLemmatizer()
for i in range(len(ws)):
  for j in range(len(ws[i])):
    ws[i][j]=lemma.lemmatize(ws[i][j]) 

data['question2_number']=ws

data.head()

ss=[]
for i in range(len(data)):
  lst=data.iloc[i,7]
  listToStr = ' '.join(map(str, lst))
  ss.append(listToStr)
data['text_number']=ss

ss=[]
for i in range(len(data)):
  lst=data.iloc[i,8]
  listToStr = ' '.join(map(str, lst))
  ss.append(listToStr)
data['question1_number']=ss

ss=[]
for i in range(len(data)):
  lst=data.iloc[i,9]
  listToStr = ' '.join(map(str, lst))
  ss.append(listToStr)
data['question2_number']=ss

data.head()

x_train,x_test,y_train,y_test = train_test_split(data.text_number,data.is_duplicate,test_size = 0.2 , random_state = 0)

cv=CountVectorizer(min_df=0,max_df=1,binary=False,ngram_range=(1,3))
#transformed train reviews
cv_train_reviews=cv.fit_transform(x_train)
#transformed test reviews
cv_test_reviews=cv.transform(x_test)

print('BOW_cv_train:',cv_train_reviews.shape)
print('BOW_cv_test:',cv_test_reviews.shape)

tv=TfidfVectorizer(min_df=0,max_df=1,use_idf=True,ngram_range=(1,3))
#transformed train reviews
tv_train_reviews=tv.fit_transform(x_train)
#transformed test reviews
tv_test_reviews=tv.transform(x_test)
print('Tfidf_train:',tv_train_reviews.shape)
print('Tfidf_test:',tv_test_reviews.shape)

lr=LogisticRegression(penalty='l2',max_iter=500,C=1,random_state=0)
#Fitting the model for Bag of words
lr_bow=lr.fit(cv_train_reviews,y_train)
print(lr_bow)
#Fitting the model for tfidf features
lr_tfidf=lr.fit(tv_train_reviews,y_train)
print(lr_tfidf)

#Predicting the model for bag of words
lr_bow_predict=lr.predict(cv_test_reviews)
##Predicting the model for tfidf features
lr_tfidf_predict=lr.predict(tv_test_reviews)
#Accuracy score for bag of words
lr_bow_score=accuracy_score(y_test,lr_bow_predict)
print("lr_bow_score :",lr_bow_score)
#Accuracy score for tfidf features
lr_tfidf_score=accuracy_score(y_test,lr_tfidf_predict)
print("lr_tfidf_score :",lr_tfidf_score)

#training the model
mnb=MultinomialNB()
#fitting the nb for bag of words
mnb_bow=mnb.fit(cv_train_reviews,y_train)
print(mnb_bow)
#fitting the nb for tfidf features
mnb_tfidf=mnb.fit(tv_train_reviews,y_train)
print(mnb_tfidf)

#Predicting the model for bag of words
mnb_bow_predict=mnb.predict(cv_test_reviews)
#Predicting the model for tfidf features
mnb_tfidf_predict=mnb.predict(tv_test_reviews)
#Accuracy score for bag of words
mnb_bow_score=accuracy_score(y_test,mnb_bow_predict)
print("mnb_bow_score :",mnb_bow_score)
#Accuracy score for tfidf features
mnb_tfidf_score=accuracy_score(y_test,mnb_tfidf_predict)
print("mnb_tfidf_score :",mnb_tfidf_score)

model1 = Sequential()
model1.add(Dense(units = 75 , activation = 'relu' , input_dim = cv_train_reviews.shape[1]))
model1.add(Dense(units = 50 , activation = 'relu'))
model1.add(Dense(units = 25 , activation = 'relu'))
model1.add(Dense(units = 10 , activation = 'relu')) 
model1.add(Dense(units = 1 , activation = 'sigmoid'))
model1.compile(optimizer = 'adam' , loss = 'binary_crossentropy' , metrics = ['accuracy'])

model1.train_on_batch(cv_train_reviews,y_train)

cnn_test_accuracy=model1.test_on_batch(cv_test_reviews,y_test)

cnn_test_accuracy[0]

from xgboost import XGBClassifier

# XGBoost Classifier model3
model2=XGBClassifier(learning_rate=0.1, n_estimators=140, max_depth=5,
                        min_child_weight=3, gamma=0.2, subsample=0.6, colsample_bytree=1.0,
                        objective='binary:logistic', nthread=4, scale_pos_weight=1, seed=27)

model2.fit(tv_train_reviews,y_train,eval_metric='auc')

preds=model2.predict(tv_test_reviews)

scoress=accuracy_score(y_test,preds)
print("score :",scoress)

from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier(n_estimators = 800,min_samples_leaf = 10,min_samples_split=15)

rf.fit(tv_train_reviews,y_train)

y = rf.predict(tv_test_reviews)

rff=accuracy_score(y_test,y)
print("mnb_bow_score :",rff)

data.head()

tokenizer = Tokenizer()
tokenizer.fit_on_texts(data.text)
X = tokenizer.texts_to_sequences(data.text)
data['words'] = X

data.head()

glove_path='/content/drive/My Drive/glove.6B.50d.txt'

EMBEDDING_DIM = 50

embeddings_index = {}
f = open(glove_path)
for line in f:
    values = line.split()
    word = values[0]
    coefs = np.asarray(values[1:], dtype='float32')
    embeddings_index[word] = coefs
f.close()

#print('Found %s unique tokens.' % len(word_index))
print('Total %s word vectors.' % len(embeddings_index))

maxlen = 50
X = list(sequence.pad_sequences(data.words, maxlen=maxlen))

word_index = tokenizer.word_index

unique_words=[*word_index]
unique_words.insert(0, ' ') 
len(unique_words)

embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))
for word, i in word_index.items():
    embedding_vector = embeddings_index.get(word)
    if embedding_vector is not None:
        embedding_matrix[i] = embedding_vector

embedding_layer = Embedding(len(word_index)+1,
                            EMBEDDING_DIM,
                            embeddings_initializer=Constant(embedding_matrix),
                            input_length=maxlen,
                            trainable=False)

embedding_matrix.shape

pd.DataFrame(embedding_matrix, index=unique_words)

X = np.array(X)
Y = np_utils.to_categorical(list(data.is_duplicate))

seed = 29
x_train, x_val, y_train, y_val = train_test_split(X, Y, test_size=0.2, random_state=seed)

inp = Input(shape=(maxlen,), dtype='int32')
x = embedding_layer(inp)
x = Bidirectional(LSTM(128, return_sequences=True, name="BiLSTM-1",recurrent_regularizer=l2(0.01)))(x)
x = Dropout(0.5, name="Dropout-1")(x)
x = Bidirectional(LSTM(128, name="BiLSTM-2",recurrent_regularizer=l2(0.01)))(x)
x = Dropout(0.5, name="Dropout-2")(x)
outp = Dense(2, activation='softmax', name="FC-layer")(x)
model = Model(inp, outp)
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])
model.summary()

model_history = model.fit(x_train, y_train, batch_size=128, epochs=15, validation_data=(x_val, y_val))

acc = model_history.history['acc']
val_acc = model_history.history['val_acc']
loss = model_history.history['loss']
val_loss = model_history.history['val_loss']
epochs = range(1, len(acc) + 1)
val_acc1=max(val_acc)

plt.title('Training and validation accuracy')
plt.plot(epochs, acc, 'red', label='Training acc')
plt.plot(epochs, val_acc, 'blue', label='Validation acc')
plt.legend()

plt.figure()
plt.title('Training and validation loss')
plt.plot(epochs, loss, 'red', label='Training loss')
plt.plot(epochs, val_loss, 'blue', label='Validation loss')
plt.legend()
plt.show()

tokenizer = Tokenizer()
tokenizer.fit_on_texts(data.text)
X = tokenizer.texts_to_sequences(data.text)
data['words'] = X

X = tokenizer.texts_to_sequences(data.question2)
data['words_quesion2'] = X

X = tokenizer.texts_to_sequences(data.question1)
data['words_quesion1'] = X

data.head()

maxlen = 50
X1 = list(sequence.pad_sequences(data.words_quesion1, 50))
X2 = list(sequence.pad_sequences(data.words_quesion2,50))

word_index = tokenizer.word_index

len(word_index)

unique_words=[*word_index]
unique_words.insert(0, ' ') 
len(unique_words)

embedding_matrix = np.zeros((len(word_index) + 1, 50))
for word, i in word_index.items():
    embedding_vector = embeddings_index.get(word)
    if embedding_vector is not None:
        embedding_matrix[i] = embedding_vector

embedding_layer = Embedding(len(word_index)+1,
                            50,
                            embeddings_initializer=Constant(embedding_matrix),
                            input_length=maxlen,
                            trainable=False)

len(X1)

X1_train=X1[:8000]
X1_test=X1[8000:]
X2_train=X2[:8000]
X2_test=X2[8000:]

X1_train = np.array(X1_train)
X2_train = np.array(X2_train)
X1_test = np.array(X1_test)
X2_test = np.array(X2_test)
Y = np_utils.to_categorical(list(data.is_duplicate))
Y_train=Y[:8000]
Y_test=Y[8000:]

def cosine_distance(vests):
    x, y = vests
    x = K.l2_normalize(x, axis=-1)
    y = K.l2_normalize(y, axis=-1)
    return -K.mean(x * y, axis=-1, keepdims=True)

def cos_dist_output_shape(shapes):
    shape1, shape2 = shapes
    return (shape1[0],1)

input_1 = Input(shape=(maxlen,), dtype='int32')
input_2 = Input(shape=(maxlen,), dtype='int32')

lstm_1 = embedding_layer(input_1)
lstm_2 = embedding_layer(input_2)


common_lstm = LSTM(64,return_sequences=True, activation="relu",recurrent_regularizer=l2(0.5))
vector_1 = common_lstm(lstm_1)
vector_1 = Flatten()(vector_1)

vector_2 = common_lstm(lstm_2)
vector_2 = Flatten()(vector_2)

x3 = Subtract()([vector_1, vector_2])
x3 = Multiply()([x3, x3])

x1_ = Multiply()([vector_1, vector_1])
x2_ = Multiply()([vector_2, vector_2])
x4 = Subtract()([x1_, x2_])
    
    #https://stackoverflow.com/a/51003359/10650182
x5 = Lambda(cosine_distance, output_shape=cos_dist_output_shape)([vector_1, vector_2])
    
conc = Concatenate(axis=-1)([x5,x4, x3])

x = Dense(100, activation="relu", name='conc_layer')(conc)
x = Dropout(0.01)(x)
out = Dense(2, activation="sigmoid", name = 'out')(x)

model = Model([input_1, input_2], out)
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])

model.summary()

from tensorflow.keras.utils import  plot_model
plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)

model_history1 = model.fit([X1_train,X2_train], Y_train, batch_size=128, epochs=7, validation_data=([X1_test,X2_test], Y_test))

acc = model_history1.history['acc']
val_acc = model_history1.history['val_acc']
loss = model_history1.history['loss']
val_loss = model_history1.history['val_loss']
epochs = range(1, len(acc) + 1)
val_acc1=max(val_acc)

plt.title('Training and validation accuracy')
plt.plot(epochs, acc, 'red', label='Training acc')
plt.plot(epochs, val_acc, 'blue', label='Validation acc')
plt.legend()

plt.figure()
plt.title('Training and validation loss')
plt.plot(epochs, loss, 'red', label='Training loss')
plt.plot(epochs, val_loss, 'blue', label='Validation loss')
plt.legend()
plt.show()

from wordcloud import WordCloud
plt.figure(figsize = (20,20)) # Text Reviews with Poor Ratings
wc = WordCloud(min_font_size = 3,  max_words = 2000 , width = 1600 , height = 800).generate(" ".join(unique_words))
plt.imshow(wc,interpolation = 'bilinear')

acc=dict({'LSTM':70.10,'CNN':69.10,'LR-TF_IDF':62.6,'LR-BOW':67.55,'MNB-TF_IDF':67.75,'MNB-BOW':68.05,'XGB':62.45,'RF':62.45,'Siamese':70.15})
acc

fig = plt.figure(figsize=(21,5))

#  subplot #1
fig.add_subplot(1,3,1)
sns.barplot(x=[*acc],y=list(acc.values())).set_title('Validation_accuracy')

#  subplot #2
fig.add_subplot(1,3,2)
sns.lineplot(x=[*acc],y=list(acc.values())).set_title('Validation_accuracy')

#  subplot #3
fig.add_subplot(1,3,3)
sns.boxplot(x=[*acc],y=list(acc.values())).set_title('Validation_accuracy')
plt.tight_layout()

